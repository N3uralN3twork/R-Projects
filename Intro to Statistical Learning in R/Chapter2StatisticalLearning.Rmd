---
title: "Chapter 2 Statistical Learning"
author: "Matthias Quinn"
date: "5/10/2020"
output: html_document
---

```{r}
library(ISLR)
```

Two routes:

* Inference

* Prediction

Two ways to predict $f(x)$:
 
* Parametric:
  
  - Makes assumptions about the shape of the data
  
  - Requires methods to fit training data to the model

* Non-parametric:

  - Do not make any assumptions about the form of the function
  
  - Require a large number of observatiosn to be accurate
  
Three types of learning:

* Supervised:
 
  - labels provided

* Unsupervised:
  
  - no response given
  
* Semi-supervised
  
Two types of variables:

* Quantitative (Numeric)

* Qualitative (Categorical)

*No Free Lunch Theorem:*

**There is no one method that dominates over all other methods for any particular dataset**
  
  
Assessing the Quality of Fit:

Most common metric is called the Mean Square Error (MSE):

$MSE = (\frac{1}{n})\sum_{i=1}^{n}(y_{i} - x_{i})^{2}$
  
We'd like to select the model for whicch the average test MSE is as small as possible.

$Ave(y_{0}-\hat{f}(x_{0}))^{2}$
  
  
When a given method yields a smalll training MSE but a large testing MSE, you are probably overfitting the data.

## The Bias-Variance Tradeoff

Variance:
  
* The amount by which $\hat{f}$ would change if you used different trainning data

* In general, more flexible methods have higher variance

Bias:

* The error introduced by approximating a complex problem by using a simple model

## The Classification Setting

The most common approach for quantifying the accuracy of our estimate $\hat{f}$ is the training *error rate*, the proportion of mistakes that are made if we apply our estimate to the training observations:

$(\frac{1}{n})\sum_{i=1}^{n}I(y_i\ne\hat{y_i})$

  
A good classifier has the smallest test error rate.

### The Bayes Classifier

Assigns each observation to the most likely class, given its predictor values.

$P(Y=j|X=x_0)$, a *conditional probability*

THe probability that $Y=j$ given the observed predictor vector is $x_0$.

The Bayes classifier produces the lowest possible test error rate, called the *Bayes error rate*.
  
The overall Bayes error rate is given by:

$1-E[max(P(Y=j|X))]$, which is anagolous to the irreducible error rate discussed earlier.

### K-Nearest Neighbors

For real data, we do not know the conditional distribution of Y given X, so computing the Bayes classifier is impossible.
  
Many approaches attempt to estimate the conditional distribution of Y given X, and then classify a given observation to the class with the maximum estimated probability.

Given a positive integer $K$ and a test observation $x_0$, the KNN classifier first identifies teh $K$ points in the training data that are closest to $x_0$, represented by $N_0$ It then estimates the conditional probability for class $j$ as the fraction of points in $N_0$ whose respsonse values equal $j$:

$P(Y=j|X=x_{0}) = \frac{1}{K}\sum{I(y_{i}=j)}$

Finally, KNN applies Bayes rule and classifies the test observation $x_{0}$ to the class with tthe largest probability.


Despite being a simple approach, KNN can often produce classifiers that are surprisingly close to the optimal Bayes classifier.


The choice of $K$ has a drastic effect on the KNN classifier obtained. 

Lower values of $K$ correspond to a low bias but higher variance classifier

Higher values of $K$ correpsond to high bias but lower variance classifier

Choosing the correct level of flexibility is critical to the success of any statistical learning method.

## Exercises:

2.
  
  (a) Regression problem, Inference, n = 500, p = 4
  
  (b) Classification problem, Prediction, n = 20, p = 14
  
  (c) Regression problem, Both, n = 52, p = 4
  
4.

  (a) Predicting breast cancer, predicting attrition, predicting plant species
  
  (b) Predicting income, exchange rate, or future house prices
  
  (c) Clustering gene expressions, marketing data, hospital patients
  
6. Parametric statistical learning has assumptions, typically less flexible than there non-parametric counterparts.


8. 

(a)

```{r}
setwd("C:/Users/miqui/OneDrive/R Projects/Intro to Statistical Learning in R")
college <- read.csv("Datasets/College.csv", header = TRUE)
```

(c)

  i.

```{r}
summary(college)
```

  ii.
  
```{r}
pairs(college[,1:10])
```


  iii.
  
```{r}
boxplot(college$Outstate, college$Private)
```


  iv.

```{r}
Elite = rep("No", nrow(college))
Elite[college$Top10perc > 50]="Yes"
Elite = as.factor(Elite)
college = data.frame(college, Elite)
summary(college)
```


  










  
  
  
  
  