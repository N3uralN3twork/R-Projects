class4_ind = which(df$class == 4)
class5_ind = which(df$class == 5)
class6_ind = which(df$class == 6)
class7_ind = which(df$class == 7)
class1_samp = sample(class1_ind, min(n, length(class1_ind)), replace = F)
class2_samp = sample(class2_ind, min(n, length(class2_ind)), replace = F)
class3_samp = sample(class3_ind, min(n, length(class3_ind)), replace = F)
class4_samp = sample(class4_ind, min(n, length(class4_ind)), replace = F)
class5_samp = sample(class5_ind, min(n, length(class5_ind)), replace = F)
class6_samp = sample(class6_ind, min(n, length(class6_ind)), replace = F)
class7_samp = sample(class7_ind, min(n, length(class7_ind)), replace = F)
indices = c(class1_samp, class2_samp, class3_samp, class4_samp, class5_samp, class6_samp, class7_samp)
training = df[indices, ]
testing = df[-indices, ]
n = 5000
df$class <- df$Cover_Type
class1_ind = which(df$class == 1)
class2_ind = which(df$class == 2)
class3_ind = which(df$class == 3)
class4_ind = which(df$class == 4)
class5_ind = which(df$class == 5)
class6_ind = which(df$class == 6)
class7_ind = which(df$class == 7)
class1_samp = sample(class1_ind, min(n, length(class1_ind)), replace = F)
class2_samp = sample(class2_ind, min(n, length(class2_ind)), replace = F)
class3_samp = sample(class3_ind, min(n, length(class3_ind)), replace = F)
class4_samp = sample(class4_ind, min(n, length(class4_ind)), replace = F)
class5_samp = sample(class5_ind, min(n, length(class5_ind)), replace = F)
class6_samp = sample(class6_ind, min(n, length(class6_ind)), replace = F)
class7_samp = sample(class7_ind, min(n, length(class7_ind)), replace = F)
indices = c(class1_samp, class2_samp, class3_samp, class4_samp, class5_samp,             class6_samp, class7_samp)
training <- df[indices, ]
testing <- df[-indices, ]
table(training$class)
dim(training)
dim(testing)
table(testing$class)
n = 5000
df$Class <- df$Cover_Type
class1_ind = which(df$Class == 1)
class2_ind = which(df$Class == 2)
class3_ind = which(df$Class == 3)
class4_ind = which(df$Class == 4)
class5_ind = which(df$Class == 5)
class6_ind = which(df$Class == 6)
class7_ind = which(df$Class == 7)
class1_samp = sample(class1_ind, min(n, length(class1_ind)), replace = F)
class2_samp = sample(class2_ind, min(n, length(class2_ind)), replace = F)
class3_samp = sample(class3_ind, min(n, length(class3_ind)), replace = F)
class4_samp = sample(class4_ind, min(n, length(class4_ind)), replace = F)
class5_samp = sample(class5_ind, min(n, length(class5_ind)), replace = F)
class6_samp = sample(class6_ind, min(n, length(class6_ind)), replace = F)
class7_samp = sample(class7_ind, min(n, length(class7_ind)), replace = F)
indices = c(class1_samp, class2_samp, class3_samp, class4_samp, class5_samp,             class6_samp, class7_samp)
training <- df[indices, ]
testing <- df[-indices, ]
# Subtract by 1, since multi-class starts at 0 rn.
training$Cover_Type <- as.integer(training$Class) - 1
testing$Cover_Type <- as.integer(testing$Class) - 1
xTrain <- training %>%
select(-Cover_Type, -CoverName, -class) %>%
as.matrix()
yTrain <- training %>%
select(Cover_Type) %>%
as.matrix()
xTest <- testing %>%
select(-Cover_Type, -CoverName, -class) %>%
as.matrix()
yTest <- testing %>%
select(Cover_Type) %>%
as.matrix()
xgbTrain <- xgb.DMatrix(data=xTrain, label=yTrain)
xgbTest <- xgb.DMatrix(data=xTest, label=yTest)
# Define the parameters for multinomial classification
numClasses = length(levels(as.factor(df$Cover_Type)))
params = list(
booster="gbtree",
eta=0.001,
max_depth=5,
gamma=3,
subsample=0.75,
colsample_bytree=1,
objective="multi:softprob",
eval_metric="auc",
num_class=numClasses
)
# Define the parameters for multinomial classification
numClasses = length(levels(as.factor(df$Class)))
params = list(
booster="gbtree",
eta=0.001,
max_depth=5,
gamma=3,
subsample=0.75,
colsample_bytree=1,
objective="multi:softprob",
eval_metric="auc",
num_class=numClasses
)
set.seed(1234)
xgbMod2 <- xgb.train(
params = params,
data = xgbTrain,
nrounds = 10,
early_stopping_rounds = 2,
watchlist = list(val1 = xgb.train, val2 = xgb.test),
verbose = 0
)
set.seed(1234)
xgbMod2 <- xgb.train(
params = params,
data = xgbTrain,
nrounds = 10,
early_stopping_rounds = 2,
watchlist = list(val1 = xgbTrain, val2 = xgbTest),
verbose = 0
)
rm(df)
rm(list = ls())
knitr::opts_chunk$set(echo = TRUE)
df <- read_csv("df.csv", col_names = TRUE, progress = TRUE)
set.seed(1234)
inTrain <- createDataPartition(df$Cover_Type,
p = .75,
list = FALSE,
times = 1)
training <- df[inTrain, ]
testing <- df[-inTrain, ]
n = 5000
df$Class <- df$Cover_Type
class1_ind = which(df$Class == 1)
class2_ind = which(df$Class == 2)
class3_ind = which(df$Class == 3)
class4_ind = which(df$Class == 4)
class5_ind = which(df$Class == 5)
class6_ind = which(df$Class == 6)
class7_ind = which(df$Class == 7)
class1_samp = sample(class1_ind, min(n, length(class1_ind)), replace = F)
class2_samp = sample(class2_ind, min(n, length(class2_ind)), replace = F)
class3_samp = sample(class3_ind, min(n, length(class3_ind)), replace = F)
class4_samp = sample(class4_ind, min(n, length(class4_ind)), replace = F)
class5_samp = sample(class5_ind, min(n, length(class5_ind)), replace = F)
class6_samp = sample(class6_ind, min(n, length(class6_ind)), replace = F)
class7_samp = sample(class7_ind, min(n, length(class7_ind)), replace = F)
indices = c(class1_samp, class2_samp, class3_samp, class4_samp, class5_samp,             class6_samp, class7_samp)
training <- df[indices, ]
testing <- df[-indices, ]
# Subtract by 1, since multi-class starts at 0 rn.
training$Cover_Type <- as.integer(training$Class) - 1
testing$Cover_Type <- as.integer(testing$Class) - 1
xTrain <- training %>%
select(-Cover_Type, -CoverName, -class) %>%
as.matrix()
# Subtract by 1, since multi-class starts at 0 rn.
training$Cover_Type <- as.integer(training$Class) - 1
testing$Cover_Type <- as.integer(testing$Class) - 1
xTrain <- training %>%
select(-Cover_Type, -CoverName, -Class) %>%
as.matrix()
yTrain <- training %>%
select(Cover_Type) %>%
as.matrix()
xTest <- testing %>%
select(-Cover_Type, -CoverName, -Class) %>%
as.matrix()
yTest <- testing %>%
select(Cover_Type) %>%
as.matrix()
xgbTrain <- xgb.DMatrix(data=xTrain, label=yTrain)
xgbTest <- xgb.DMatrix(data=xTest, label=yTest)
# Define the parameters for multinomial classification
numClasses = length(levels(as.factor(df$Class)))
params = list(
booster="gbtree",
eta=0.001,
max_depth=5,
gamma=3,
subsample=0.75,
colsample_bytree=1,
objective="multi:softprob",
eval_metric="auc",
num_class=numClasses
)
set.seed(1234)
xgbMod2 <- xgb.train(
params = params,
data = xgbTrain,
nrounds = 10,
early_stopping_rounds = 2,
watchlist = list(val1 = xgbTrain, val2 = xgbTest),
verbose = 0
)
table(yTrain)
table(yTest)
# Define the parameters for multinomial classification
numClasses = length(levels(as.factor(df$Class)))
params = list(
booster="gbtree",
eta=0.001,
max_depth=5,
gamma=3,
colsample_bytree=1,
objective="multi:softprob",
eval_metric="auc",
num_class=numClasses
)
set.seed(1234)
xgbMod2 <- xgb.train(
params = params,
data = xgbTrain,
nrounds = 10,
early_stopping_rounds = 2,
watchlist = list(val1 = xgbTrain, val2 = xgbTest),
verbose = 0
)
# Define the parameters for multinomial classification
numClasses = length(levels(as.factor(df$Class)))
params = list(
booster="gbtree",
eta=0.001,
max_depth=5,
gamma=3,
subsample=0.75,
colsample_bytree=1,
objective="multi:softprob",
eval_metric="auc",
num_class=numClasses
)
# Define the parameters for multinomial classification
numClasses = length(levels(as.factor(df$Class)))
params = list(
booster="gbtree",
eta=0.001,
max_depth=5,
gamma=3,
subsample=0.75,
colsample_bytree=1,
objective="multi:softprob",
eval_metric="acc",
num_class=numClasses
)
set.seed(1234)
xgbMod2 <- xgb.train(
params = params,
data = xgbTrain,
nrounds = 10,
early_stopping_rounds = 2,
watchlist = list(val1 = xgbTrain, val2 = xgbTest),
verbose = 0
)
# Define the parameters for multinomial classification
numClasses = length(levels(as.factor(df$Class)))
params = list(
booster="gbtree",
eta=0.001,
max_depth=5,
gamma=3,
subsample=0.75,
colsample_bytree=1,
objective="multi:softprob",
eval_metric="accuracy",
num_class=numClasses
)
set.seed(1234)
xgbMod2 <- xgb.train(
params = params,
data = xgbTrain,
nrounds = 10,
early_stopping_rounds = 2,
watchlist = list(val1 = xgbTrain, val2 = xgbTest),
verbose = 0
)
# Define the parameters for multinomial classification
numClasses = length(levels(as.factor(df$Class)))
params = list(
booster="gbtree",
eta=0.001,
max_depth=5,
gamma=3,
subsample=0.75,
colsample_bytree=1,
objective="multi:softprob",
eval_metric="merror", # Multi-class classification error rate
num_class=numClasses
)
set.seed(1234)
xgbMod2 <- xgb.train(
params = params,
data = xgbTrain,
nrounds = 10,
early_stopping_rounds = 2,
watchlist = list(val1 = xgbTrain, val2 = xgbTest),
verbose = 0
)
# Review the final model and results:
xgbMod2
xgbPreds <- predict(xgbMod2, xTest, reshape = TRUE)
xgbPreds <- as.data.frame(xgbPreds)
colnames(xgbPreds) <- levels(as.factor(df$CoverName))
xgbPreds$Prediction <- apply(xgbPreds, 1, which.max)
xgbPreds$Label <- yTest - 1
accuracy = sum(xgbPreds$Prediction == xgbPreds$Label) / nrow(xgbPreds)
print(paste("Final Accuracy =", sprintf("%1.3f%%", 100 * accuracy)))
# Review the final model and results:
xgbMod2
# Define the parameters for multinomial classification
numClasses = length(levels(as.factor(df$Class)))
params = list(
booster="gbtree",
eta=0.001,
max_depth=10,
gamma=3,
subsample=0.75,
colsample_bytree=1,
objective="multi:softprob",
eval_metric="merror", # Multi-class classification error rate
num_class=numClasses
)
set.seed(1234)
xgbMod2 <- xgb.train(
params = params,
data = xgbTrain,
nrounds = 10,
early_stopping_rounds = 2,
watchlist = list(val1 = xgbTrain, val2 = xgbTest),
verbose = 0
)
# Review the final model and results:
xgbMod2
# Define the parameters for multinomial classification
numClasses = length(levels(as.factor(df$Class)))
params = list(
booster="gbtree",
eta=0.001,
max_depth= c(5, 10),
gamma=3,
subsample=0.75,
colsample_bytree=1,
objective="multi:softprob",
eval_metric="merror", # Multi-class classification error rate
num_class=numClasses
)
set.seed(1234)
xgbMod2 <- xgb.train(
params = params,
data = xgbTrain,
nrounds = 10,
early_stopping_rounds = 2,
watchlist = list(val1 = xgbTrain, val2 = xgbTest),
verbose = 0
)
# Review the final model and results:
xgbMod2
# Review the final model and results:
xgbMod2$params
# Review the final model and results:
xgbMod2$best_score
# Define the parameters for multinomial classification
numClasses = length(levels(as.factor(df$Class)))
params = list(
booster="gbtree",
eta=0.001,
max_depth= c(10),
gamma=3,
subsample=0.75,
colsample_bytree=1,
objective="multi:softprob",
eval_metric="merror", # Multi-class classification error rate
num_class=numClasses
)
set.seed(1234)
xgbMod2 <- xgb.train(
params = params,
data = xgbTrain,
nrounds = 10,
early_stopping_rounds = 2,
watchlist = list(val1 = xgbTrain, val2 = xgbTest),
verbose = 0
)
# Review the final model and results:
xgbMod2
xgbPreds <- predict(xgbMod2, xTest, reshape = TRUE)
xgbPreds <- as.data.frame(xgbPreds)
colnames(xgbPreds) <- levels(as.factor(df$CoverName))
xgbPreds$Prediction <- apply(xgbPreds, 1, which.max)
xgbPreds$Label <- yTest - 1
confusionMatrix(data = xgbPreds$Prediction, reference = xgbPreds$Label, mode = "everything")
table(xgbPreds$Prediction)
table(xgbPreds$Label)
table(yTest)
xgbPreds <- predict(xgbMod2, xgbTest, reshape = TRUE)
xgbPreds <- as.data.frame(xgbPreds)
colnames(xgbPreds) <- levels(as.factor(df$CoverName))
View(xgbPreds)
xgbPreds <- predict(xgbMod2, xgbTest, reshape = TRUE)
xgbPreds <- as.data.frame(xgbPreds)
colnames(xgbPreds) <- levels(as.factor(df$CoverName))
xgbPreds$Prediction <- apply(xgbPreds, 1, which.max)
xgbPreds$Label <- yTest
confusionMatrix(data = xgbPreds$Prediction, reference = xgbPreds$Label)
table(xgbPreds$Prediction)
table(xgbPreds$Label)
xgbPreds <- predict(xgbMod2, xgbTest, reshape = TRUE)
xgbPreds <- as.data.frame(xgbPreds)
colnames(xgbPreds) <- levels(as.factor(df$CoverName))
xgbPreds$Prediction <- apply(xgbPreds, 1, which.max)
xgbPreds$Label <- yTest
xgbPreds$Prediction
table(xgbPreds$Prediction)
table(xgbPreds$Label)
labels <- getinfo(xgbTest, "label")
labels <- getinfo(xgbTest, "label")
preds <- predict(xgbMod2, xgbTest)
labels <- getinfo(xgbTest, "label")
preds <- predict(xgbMod2, xgbTest)
errorRate <- as.numeric(sum(as.integer(pred > (1/7) != labels)) / length(labels))
labels <- getinfo(xgbTest, "label")
preds <- predict(xgbMod2, xgbTest)
errorRate <- as.numeric(sum(as.integer(pred > (1/7)) != labels)) / length(labels))
labels <- getinfo(xgbTest, "label")
preds <- predict(xgbMod2, xgbTest)
errorRate <- as.numeric(sum(as.integer(pred > (1/7)) != labels)) / length(labels)
labels <- getinfo(xgbTest, "label")
preds <- predict(xgbMod2, xgbTest)
errorRate <- as.numeric(sum(as.integer(preds > (1/7)) != labels)) / length(labels)
print(paste("Test-error Rate=", errorRate))
labels <- getinfo(xgbTest, "label")
preds <- predict(xgbMod2, xgbTest)
errorRate <- as.numeric(sum(as.integer(preds > 0.5) != labels)) / length(labels)
print(paste("Test-error Rate=", errorRate))
View(xgbPreds)
xgbPreds <- predict(xgbMod2, xgbTest, reshape = TRUE)
xgbPreds <- as.data.frame(xgbPreds)
colnames(xgbPreds) <- levels(as.factor(df$CoverName))
xgbPreds$Prediction <- apply(xgbPreds, 1, which.max)
xgbPreds$Label <- yTest + 1
confusionMatrix(data = xgbPreds$Prediction, reference = xgbPreds$Label)
rm(list = ls())
install.packages(c("backports", "BH", "brio", "broom", "cli", "conflicted", "cpp11", "credentials", "datawizard", "DBI", "devtools", "DiagrammeR", "digest", "dtplyr", "effectsize", "Exact", "fansi", "fs", "gld", "glue", "hardhat", "httpuv", "insight", "jsonlite", "knitr", "lavaan", "memoise", "multcomp", "nlme", "openssl", "parallelly", "parameters", "pillar", "pkgbuild", "pkgload", "progressr", "questionr", "raster", "rbibutils", "rcompanion", "Rcpp", "Rdpack", "readr", "remotes", "rex", "rjson", "sessioninfo", "sf", "spdep", "stringi", "terra", "testthat", "tinytex", "usethis", "vroom", "withr", "wk", "xfun", "xml2", "yaml"))
colors <- c("red", "red", "green", "green", "green", "orange")
table(colors)
setwd("C:/Users/miqui/OneDrive/R Projects/Map Making/Philidelphia")
library(sf)
# Read in the shape files:
PhilCrimes <- st_read("Data/PhillyCrimerate")
ggplot(PhilCrimes) +
geom_sf(aes(fill = homic_rate))
library(dplyr)
library(ggplot2)
library(tigris)
library(sf)
library(leaflet)
library(RColorBrewer)
library(classInt)
# Read in the shape files:
PhilCrimes <- st_read("Data/PhillyCrimerate")
ggplot(PhilCrimes) +
geom_sf(aes(fill = homic_rate))
philly_WGS84 <- st_transform(PhilCrimes, 4326)
head(philly_WGS84)
names(philly_WGS84)
names(attributes(philly_WGS84))
rm(list = ls())
setwd("C:/Users/miqui/OneDrive/R Projects/Map Making/Philidelphia")
"
Source: https://cengel.github.io/R-spatial/mapping.html
Shape files: https://www.census.gov/cgi-bin/geo/shapefiles/index.php
"
# Load in the necessary libraries:
library(dplyr)
library(ggplot2)
library(tigris)
library(sf)
library(leaflet)
library(RColorBrewer)
library(classInt)
# Read in the shape files:
PhilCrimes <- st_read("Data/PhillyCrimerate")
# 1st plot of Homicide rate using built-in `plot` function:
plot(PhilCrimes["homic_rate"],
breaks = "quantile")
# 2nd plot using ggplot2:
ggplot(PhilCrimes) +
geom_sf(aes(fill = homic_rate))
# 3rd plot using Leaflet:
"Reproject the current sf data:"
philly_WGS84 <- st_transform(PhilCrimes, 4326)
pal_fun <- colorQuantile("YlOrRd", NULL, n = 5)
p_popup <- paste0("<strong>Homicide Density: </strong>", philly_WGS84$homic_rate)
# quantile breaks
breaks_qt <- classIntervals(PhilCrimes$homic_rate, n = 7, style = "quantile")
leaflet(philly_WGS84) %>%
addPolygons(
stroke = FALSE,
fillColor = ~pal_fun(homic_rate),
fillOpacity = 0.8, smoothFactor = 0.5,
popup = p_popup,
group = "philly") %>%
addTiles(group = "OSM") %>%
addProviderTiles("CartoDB.DarkMatter", group = "Carto") %>%
addLegend("bottomright",
colors = brewer.pal(7, "YlOrRd"),
labels = paste0("up to ", format(breaks_qt$brks[-1], digits = 2)),
title = 'Philadelphia homicide density per sqkm') %>%
addLayersControl(baseGroups = c("OSM", "Carto"),
overlayGroups = c("philly"))
View(philly_WGS84)
str(philly_WGS84)
remove.packages("yaml")
install.packages("yaml")
install.packages(c("cli", "DBI", "globals", "igraph", "performance", "processx", "ps", "ranger", "rsample", "sandwich", "see", "tinytex", "xfun"))
install.packages(c("cli", "DBI", "globals", "igraph", "performance", "processx", "ps", "ranger", "rsample", "sandwich", "see", "tinytex", "xfun"))
